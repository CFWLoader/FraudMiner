\documentclass{article}

\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\title{Overview of Streaming Graph Partitioning}
\date{2017-06-21}
\author{Evan Huang \\ cfwloader@gmail.com}

\begin{document}

	\maketitle

	\newpage

	\tableofcontents

	% \pagenumbering{gobble}
	\pagenumbering{arabic}

	\newpage
	\section{Goals}
	\subsection{Evenly partitioning graphs into distributed systems}
	\subsection{Minimizing the cost of executing the queries}
	\subsection{Fastly collecting the graph information}

	\newpage
	\section{Preliminaries, Problmes and current progresses}
	\subsection{Preliminaries}
	\subsubsection{Power Law Graph}
	\begin{equation}
		\textbf{P}(d) \propto d^{-\alpha}
	\end{equation}
	\begin{equation}\label{pwlformula}
		\alpha = \frac{|E|}{|V|}
	\end{equation}
	\subsection{Problems}
	\subsubsection{Cycle Graph}
	This problem is mentioned by Stanton \emph{et al.}\cite{stanton2012streaming}, and this paper also proposed that the best performing heuristic is a weighted variant of the greedy algorithm. There are there three typical graph streaming orders considered in this page:\newline
	1. Beneath First Search.\newline
	2. Depth First Search.\newline
	3. Random.\newline

	\subsubsection{Inseparable Problem}
	I tested the use of \textbf{Giraph}\cite{giraph} and found out that it still can not parallelly handle graph though this framework is based on the hadoop\cite{hadoop}.

	\subsection{Current Progress}
	\subsubsection{Graph Partitioning}
	METIS\cite{metis} is the main-stream algorithm to partition the graph. Though so many years passed, the efficiency is still low in my personal opinion based on the evidence\cite{salihoglu2013gps:}. But There are now more inspiration. And GPS system useses an in-memory map to record where the node stored.
	\subsubsection{Technicals}
	\textbf{Replica} is a common techs for graph partitioning. That means the cut vertexes will be copied to the cluster machines. More than one machine in the cluster will keep the replica of the node.
	\subsubsection{Partitional Heuristics}
	Stanton \emph{et al.}\cite{stanton2012streaming} also summed up 10 heuristics for partitioning.(\emph{ind} indicates the which machine the node belongs to.)\newline
	1. \textbf{Balanced} - \begin{equation} \emph{ind} = \mathop{\argmin}_{i\in[k]}{\{|P^t(i)|\}} \end{equation}
	2. \textbf{Chunking} - \begin{equation} \emph{ind} = \lceil t/C \rceil \end{equation}						% floor
	3. \textbf{Hashing} - \begin{equation} H(\emph{v}) = (\emph{v}~mod~\emph{k})~+~1 \end{equation}
	4. \textbf{(Weighted) Deterministic Greedy} - \begin{equation} \emph{ind}=\mathop{\argmax}_{i\in[k]}{\{|P^t(\emph{i})\cap\Gamma(\emph{v})|\emph{w}(\emph{t},\emph{i})\}} \end{equation}
	5. \textbf{(Weighted) Randomized Greedy} - \begin{equation} \emph{Pr(i)}=|P^t(\emph{i})\cap\Gamma(\emph{v})|\emph{w}(\emph{t},\emph{i})/Z \end{equation}
	6. \textbf{(Weighted) Triangles} - 
	\begin{equation}
		\mathop{\argmax}_{i\in[k]}{\{\frac{|\emph{E}(P^t(\emph{i})\cap\Gamma(\emph{v}), P^t(\emph{i})\cap\Gamma(\emph{v}))|}
		{\binom{|P^t(\emph{i})\cap\Gamma(\emph{v})|}{2}}\emph{w}(\emph{t},\emph{i})\}}
	\end{equation}
	7. \textbf{Balance Big}
	The following heuristics all use a buffer.\newline
	8. \textbf{Prefer Big}\newline
	9. \textbf{Avoid Big}\newline
	10.\textbf{Greedy EvoCut}\cite{andersen2009finding}


	\subsubsection{Giraph}
	Giraph is a iterative graph processing computational framework based on Map-Reduced frameworkd(Hadoop provides).
	\textbf{Related Bases}\newline
	\textbf{Hadoop.} Hadoop provides the distributive data store system. It is the main-stream open source implementation of GFS\cite{ghemawat2003the}.\newline
	\textbf{Map Reduced Framework.}\cite{lammel2008google's} This framework provides cluster-computing function. The computational process will be \textbf{\emph{mapped}} to several or more sub compu tational processes. After individual processes finished, \textbf{Bulk Synchronous Parallelism}\cite{gerbessiotis1994direct} will happens to control the iteration. All sub processes will be \text{\emph{reduced}} to the result.\newline
	But in my current experiment configurations, hadoop performs badly in the version problem. The stable and industry-used version is 0.22, the version Giraph requires is 1.2.1, HAMA\cite{seo2010hama:} requires 2.7.0 with zookeeper. Twitter Storm\cite{storm} is also such a distibutive computational framework as Giraph but requires more facilities.

	\subsubsection{GraphChi}
	\textbf{GraphChi}\cite{kyrola2012graphchi:} is a graph computation framework. Different with Map-Reduces Computation frameworks and distributive graph store system, GraphChi uses single personal computer to compute big graph(over millions of nodes). It uses \textbf{sorted source node number(ID)} to divide the graph to shards. Once we use GraphChi as the framework, that means we have to preprocess the ID of nodes.
	\newline
	\textbf{Shining Points}
	\newline
	\textbf{Evolving Graphs Supports.}
	\newline
	\textbf{Less communicating produres.} GraphChi programs are similar to those written for Pregel\cite{malewicz2009pregel:} or GraphLab\cite{low2014graphlab:}. But those programs depend on Message Passing Interface(MPI)\cite{meyer1993message-passing}, GraphChi programs directly modify the value of vertex.
	\newline
	% \subsubsection{Shortages}
	\textbf{Shortages:}
	\newline
	\textbf{1. Graph Traversal-like Algorithms.} Such this algorithms require loading neighbor nodes, that means scanning a complete memory-shard. Under such situation, GraphChi would degenerate to linear loading process.
	\newline
	\textbf{2. Big part load.} GraphChi requires full load of in-edges.
	\newline
	\textbf{3. Not automatic enough.} GraphChi would not automatically detect the available RAM of user PC. It needs explicit program parameters.

	\subsubsection{PowerGraph}
	\textbf{PowerGraph}\cite{gonzalez2012powergraph:} is a distributive version of GraphChi(as my temporary opinion). It performs well in the papers I viewed. It also uses synchronous model \emph{GAS} - \emph{Gather}, \emph{Apply}, \emph{Scatter}. The \emph{Gather} process is collecting the neighbors' information, the \emph{Apply} is applying some function to calculate the vertex's value and the \emph{Scatter} is updating the neighbors' value.

	\subsubsection{TurboGraph}
	\textbf{TurboGraph}\cite{han2013turbograph:} is a parallel IO and computation framework on a single PC. According to the paper's content, TurboGraph uses a mapping method to record the store location of vertexes.
	\newline
	\textbf{\emph{pin-and-slide}}
	\newline
	\textbf{\emph{column view}}

	\subsubsection{Pregel+}
	This is a enhanced implementation of Pregel. \cite{lu2014large-scale} evaluated main-stream graph computing system and Pregel takes the best overall performances.

	\newpage
	\section{Experiments}
	The following graph processing frameworks are simply tested and have some briefs:\newline
	1. Giraph\newline
	2. GraphChi\newline
	These have configuration problems, mostly due to the incompatible version of Hadoop:\newline
	1. HAMA\newline
	2. Storm\newline
	3. TurboGraph. Access Forbidden from server.\newline
	These are marked and waiting for being test:\newline
	1. GraphLab\newline
	2. PowerGraph\newline

	\subsection{Giraph}
	\textbf{Exceptions.} When the graph's size reaches about 100,000 nodes. \emph{OutOfMemory} exception will occur with Json format file. The reason is the map-reduced framework will just dispatch 2 JobTrackers, one for searching Workers and one for real computation. Once the workers job finished, the searching JobTrackers terminated, left the worker. In my experiments, map-reduced framework fails to dispatch multi workers. The only worker can not stand the high memory cost and OutOfMemory exception happens when only worker is loading the nodes.
	\subsection{GraphChi}
	\textbf{Result.} With the size of millions of nodes(Random Generated), GraphChi successfully deployed Page Rank\cite{pagerank} on this graphs and got the result.

	\newpage
	\section{Personal heuristics}
	\subsection{Cross adjacency matrix}
	General idea is transform the graph into a table with fixed rows and columns. The links are not only horizonal but vertical. But the main issue is how to handle the cross problem. For example, there are three links between 3 nodes. Once we stored as '1-2-3' then we how to express the direct link '1-3'?

	\subsection{Ideas spring from the output of the 'streaming'}
	Section 1 specifies the requirement of such a streaming graph handling system. And the method\cite{stanton2012streaming} purely considers the partitioning problems. Maybe we should start from the goals and the one of goals is Minimizing the cost of querying. Once we take this goal, we might not treat the partitioning problem so important. It means evenly partitioning the graph is not the key problem, it's the fastly collecting the graph information.

	\subsection{Distributive GraphChi? Or Metis + GraphChi?}

	\subsection{Thinking in graph partitioning}
	\subsubsection{Replica}
	\textbf{Replica} was mentioned in above section. But this technical is used for replicating all the cut vertexes. Why don't we change the way to use this technical? We can replicate the vertexes that have high degree(in, out) in every machine to reduce the cost of communication.

	\subsection{Viewing the graph as a sphere}
	This is my personal inspiration. What if we view the graph as a sphere in 3 dimensions or circle in 2 dimensions. The vertex, with less links, may more probable be placed to the posistion near the edge of the sphere(circle). We can start graph cutting work with such this vertexes. Then use some techs, such as random walk, to find the middle position of the sphere. The links of edge vertexes and middle position might be the base of cutting the sphere.
	\subsubsection{Can we find a fixed model to view the graph?}
	The relationships of vertexes in the graph are fixed. But when we use a visual picture to describe a graph, every can draw a different picture. So different way to describe the graph may trigger different method to paratition the graph. So can we find an efficient model to describe visualized graph? Or the most accurate visualized model to view the graph?
	\subsubsection{Can we apply some mathematical methods on such a 'sphere'?}
	Such as doing some integrals on this sphere to find the cutting hyperplane and evenly dividing this objects?

	\subsection{Reformat the Cluster Framework}
	In my opinion, the procedure of graph computation contains three types of cluster node:
	\newline
	1. Naive graph data node. These nodes are for storing the original graph data, such as vertex data and adjacency data.
	\newline
	2. Computation node. This type of nodes is for applying the algorithm to the graph and get the result. We can treat this nodes as the service server in a web service system.
	\newline
	3. State node. These nodes are functioning as the connector to type 1 and type 2 nodes. They also storing the temporary and stateful data of the algorithm. For example, \emph{Page Rank} needs the \emph{PR} value of every vertexes in last iteration. In the first iteration, \emph{State node} ask the \emph{Naive graph data node} for the adjacency data. In the subsequent iterations, \emph{Computation node} will require the last iterative \emph{PR} value of vertexes. Due to the stateful data, state nodes possibly need the synchronization.
	\newline
	\subsubsection{More}
	During the analysis of \textbf{State node}, I found out that the critical problem of such a graph system is the stateful data. It's also the critical problem of applying the procedure-oriented method to the algorithm. The philosophy of designing such a system is \textbf{decoupling}. According to the functions, the nodes in the cluster are categorized to the above 3 types. But this 3 typical functions can be merged into a single node as a simplified implementation.
	\newline
	But the problem is still the stateful data. This means we have to deal with it for parallel computation. The cost of synchonizing stateful data is vastly expensive.

	\subsection{Use the graph structure(pattern) to optimize the calculation}
	Imagine a social network, a big movie star may has many fans. In graph, we may see a big star shape in the graph. A cluster of friends, they may follow each others. In graph, we may see a dense subgraph in the network. So if once we find such a structure, we can put them in a single machine. This can leave least edge to be cut.
	\newline
	This requires the algorithm to dectect the major structure(pattern) in the network.
	\subsubsection{When encountering the streaming graph}
	This method can just be applied to static graph. But the graph may always changes. When the the edges changes, does the graph structure(pattern) changes? Currently I think we can use \emph{Minor Update} and \emph{Major Update} to fix such a problem.
	\newline
	\textbf{Minor Update} means the vertexes' info will be simply saved and the system will update the relative vertexes' infor.
	\newline
	\textbf{Major Update} means system needs to check the whole graph and may trigger the graph repartitioning.
	\newline
	So what's the time of \emph{Major Update}? Currently I read the \emph{PowerGraph}, the paper introduced the Power-Law Graph. We can use the power-law formula\eqref{pwlformula} as the threshold of \emph{Major Update}.

	\newpage
	\begin{appendix}
		\section{References}
		\bibliography{sgp_refs}
		\bibliographystyle{ieeetr}
	\end{appendix}

\end{document}
